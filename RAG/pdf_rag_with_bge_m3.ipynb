{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# PDF RAG ç³»ç»Ÿ - ä½¿ç”¨ BGE-M3 Embedding æ¨¡å‹\n",
    "\n",
    "æœ¬ Notebook å±•ç¤ºå¦‚ä½•ä½¿ç”¨ XPULink å¹³å°æ‰˜ç®¡çš„ BGE-M3 Embedding æ¨¡å‹æ„å»ºä¸€ä¸ªåŸºäº PDF æ–‡æ¡£çš„ RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿã€‚\n",
    "\n",
    "## åŠŸèƒ½ç‰¹æ€§\n",
    "- åŠ è½½å’Œå¤„ç† PDF æ–‡æ¡£\n",
    "- ä½¿ç”¨ BGE-M3 Embedding æ¨¡å‹è¿›è¡Œæ–‡æ¡£å‘é‡åŒ–\n",
    "- æ„å»ºå‘é‡ç´¢å¼•å®ç°é«˜æ•ˆæ£€ç´¢\n",
    "- åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆæ™ºèƒ½å›ç­”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒé…ç½®å’Œä¾èµ–å®‰è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…\n",
    "!pip install llama-index-core llama-index-llms-openai python-dotenv pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from typing import List, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n",
    "from llama_index.core.embeddings import BaseEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# æ£€æŸ¥ API Key\n",
    "if os.getenv('XPU_API_KEY'):\n",
    "    print(\"âœ… æ‰¾åˆ° XPU_API_KEY ç¯å¢ƒå˜é‡\")\n",
    "else:\n",
    "    raise ValueError(\"âŒ æœªæ‰¾åˆ° XPU_API_KEYã€‚è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®åå†è¿è¡Œæ­¤è„šæœ¬ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedding-class",
   "metadata": {},
   "source": [
    "## 2. å®šä¹‰ BGE-M3 Embedding ç±»\n",
    "\n",
    "BGE-M3 æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¤šè¯­è¨€ Embedding æ¨¡å‹ï¼Œæ”¯æŒä¸­è‹±æ–‡ç­‰å¤šç§è¯­è¨€ï¼Œç‰¹åˆ«é€‚åˆå¤„ç†ä¸­æ–‡æ–‡æ¡£ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bge-embedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BGEM3Embedding(BaseEmbedding):\n",
    "    \"\"\"BGE-M3 Embedding æ¨¡å‹å®ç°ï¼ˆåŸºäº OpenAI å…¼å®¹ APIï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        api_base: str = \"https://xpulink.ai/v1\",\n",
    "        api_key: Optional[str] = None,\n",
    "        model: str = \"bge-m3\",\n",
    "        embed_batch_size: int = 10,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ– BGE-M3 Embedding æ¨¡å‹\n",
    "        \n",
    "        Args:\n",
    "            api_base: XPULink API åŸºç¡€åœ°å€\n",
    "            api_key: API å¯†é’¥ï¼ˆä»ç¯å¢ƒå˜é‡è·å–ï¼‰\n",
    "            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º bge-m3\n",
    "            embed_batch_size: æ‰¹å¤„ç†å¤§å°\n",
    "        \"\"\"\n",
    "        self.api_base = api_base.rstrip('/')\n",
    "        self.api_key = api_key or os.getenv(\"XPU_API_KEY\")\n",
    "        self.model = model\n",
    "        self.embed_batch_size = embed_batch_size\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"éœ€è¦æä¾› API Key\")\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def _call_api(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"è°ƒç”¨ XPULink API è·å– embeddings\"\"\"\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {self.api_key}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            'model': self.model,\n",
    "            'input': texts\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.api_base}/embeddings\",\n",
    "                headers=headers,\n",
    "                json=data,\n",
    "                timeout=60\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            if result.get('data'):\n",
    "                return [item['embedding'] for item in result['data']]\n",
    "            else:\n",
    "                raise Exception(f\"API è¿”å›æ ¼å¼é”™è¯¯: {result}\")\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise Exception(f\"API è¯·æ±‚å¤±è´¥: {str(e)}\")\n",
    "    \n",
    "    def _get_query_embedding(self, query: str) -> List[float]:\n",
    "        \"\"\"è·å–å•ä¸ªæŸ¥è¯¢çš„ embedding\"\"\"\n",
    "        embeddings = self._call_api([query])\n",
    "        return embeddings[0] if embeddings else []\n",
    "    \n",
    "    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"æ‰¹é‡è·å–æ–‡æœ¬çš„ embeddings\"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        # åˆ†æ‰¹å¤„ç†\n",
    "        for i in range(0, len(texts), self.embed_batch_size):\n",
    "            batch = texts[i:i + self.embed_batch_size]\n",
    "            batch_embeddings = self._call_api(batch)\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "            \n",
    "            if i + self.embed_batch_size < len(texts):\n",
    "                print(f\"å·²å¤„ç† {i + len(batch)}/{len(texts)} ä¸ªæ–‡æœ¬ç‰‡æ®µ\")\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    async def _aget_query_embedding(self, query: str) -> List[float]:\n",
    "        \"\"\"å¼‚æ­¥è·å–æŸ¥è¯¢ embeddingï¼ˆå›é€€åˆ°åŒæ­¥æ–¹æ³•ï¼‰\"\"\"\n",
    "        return self._get_query_embedding(query)\n",
    "    \n",
    "    @property\n",
    "    def model_name(self) -> str:\n",
    "        return self.model\n",
    "\n",
    "print(\"âœ… BGE-M3 Embedding ç±»å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config",
   "metadata": {},
   "source": [
    "## 3. é…ç½® LlamaIndex Settings\n",
    "\n",
    "é…ç½®å…¨å±€çš„ Embedding æ¨¡å‹å’Œ LLMã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-settings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½® BGE-M3 Embedding æ¨¡å‹\n",
    "Settings.embed_model = BGEM3Embedding(\n",
    "    api_base=\"https://xpulink.ai/v1\",\n",
    "    model=\"bge-m3\",\n",
    "    embed_batch_size=10\n",
    ")\n",
    "\n",
    "# é…ç½® LLMï¼ˆç”¨äºç”Ÿæˆå›ç­”ï¼‰\n",
    "Settings.llm = OpenAI(\n",
    "    api_key=os.getenv(\"XPU_API_KEY\"),\n",
    "    api_base=\"https://www.xpulink.ai/v1\",\n",
    "    model=\"qwen3-32b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"âœ… LlamaIndex é…ç½®å®Œæˆ\")\n",
    "print(f\"  - Embedding æ¨¡å‹: {Settings.embed_model.model_name}\")\n",
    "print(f\"  - LLM æ¨¡å‹: {Settings.llm.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-docs",
   "metadata": {},
   "source": [
    "## 4. åŠ è½½ PDF æ–‡æ¡£\n",
    "\n",
    "å°† PDF æ–‡ä»¶æ”¾åœ¨ `data/` ç›®å½•ä¸‹ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨åŠ è½½å¹¶å¤„ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-pdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®æ•°æ®ç›®å½•è·¯å¾„\n",
    "DATA_DIR = \"./data/\"\n",
    "\n",
    "# ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "    print(f\"ğŸ“ å·²åˆ›å»ºæ•°æ®ç›®å½•: {DATA_DIR}\")\n",
    "    print(\"âš ï¸  è¯·å°† PDF æ–‡ä»¶æ”¾å…¥æ­¤ç›®å½•åé‡æ–°è¿è¡Œæ­¤å•å…ƒæ ¼\")\n",
    "else:\n",
    "    # åŠ è½½æ–‡æ¡£\n",
    "    try:\n",
    "        documents = SimpleDirectoryReader(\n",
    "            input_dir=DATA_DIR,\n",
    "            required_exts=[\".pdf\"]\n",
    "        ).load_data()\n",
    "        \n",
    "        if documents:\n",
    "            print(f\"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ\")\n",
    "            print(f\"\\nğŸ“„ ç¬¬ä¸€ä¸ªæ–‡æ¡£ç‰‡æ®µé¢„è§ˆ:\")\n",
    "            print(f\"  - é•¿åº¦: {len(documents[0].text)} å­—ç¬¦\")\n",
    "            print(f\"  - å†…å®¹é¢„è§ˆ: {documents[0].text[:200]}...\")\n",
    "        else:\n",
    "            print(f\"âš ï¸  æœªåœ¨ {DATA_DIR} ç›®å½•ä¸­æ‰¾åˆ° PDF æ–‡ä»¶\")\n",
    "            print(\"è¯·æ·»åŠ  PDF æ–‡ä»¶åé‡æ–°è¿è¡Œæ­¤å•å…ƒæ ¼\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m5subtav9l",
   "source": "## 4.5 é¢„æµ‹å‘é‡æ•°æ®åº“å¤§å°\n\nåœ¨æ„å»ºç´¢å¼•ä¹‹å‰ï¼Œé¢„æµ‹å‘é‡æ•°æ®åº“æ‰€éœ€çš„å­˜å‚¨ç©ºé—´ï¼Œå¸®åŠ©æ‚¨è§„åˆ’èµ„æºã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gia2gfk7uy",
   "source": "def estimate_vector_db_size(documents, embedding_dim=1024, dtype_bytes=4):\n    \"\"\"\n    é¢„æµ‹å‘é‡æ•°æ®åº“çš„å­˜å‚¨å¤§å°\n    \n    Args:\n        documents: æ–‡æ¡£åˆ—è¡¨\n        embedding_dim: å‘é‡ç»´åº¦ï¼ˆBGE-M3 é»˜è®¤ä¸º 1024ï¼‰\n        dtype_bytes: æ•°æ®ç±»å‹å­—èŠ‚æ•°ï¼ˆfloat32 ä¸º 4 å­—èŠ‚ï¼‰\n    \n    Returns:\n        dict: åŒ…å«å„é¡¹å¤§å°ç»Ÿè®¡çš„å­—å…¸\n    \"\"\"\n    if not documents:\n        return {\n            'error': 'æ²¡æœ‰æ–‡æ¡£å¯ä¾›åˆ†æ',\n            'total_size_mb': 0\n        }\n    \n    # ä¼°ç®—æ–‡æ¡£è¢«åˆ†å—çš„æ•°é‡\n    # LlamaIndex é»˜è®¤ chunk_size=1024, chunk_overlap=20\n    chunk_size = 1024\n    chunk_overlap = 20\n    effective_chunk_size = chunk_size - chunk_overlap\n    \n    total_chars = sum(len(doc.text) for doc in documents)\n    estimated_chunks = max(1, total_chars // effective_chunk_size)\n    \n    # è®¡ç®—å‘é‡å­˜å‚¨å¤§å°\n    # æ¯ä¸ª chunk éœ€è¦ä¸€ä¸ª embedding å‘é‡\n    vector_size_bytes = estimated_chunks * embedding_dim * dtype_bytes\n    vector_size_mb = vector_size_bytes / (1024 * 1024)\n    \n    # è®¡ç®—æ–‡æœ¬å­˜å‚¨å¤§å°ï¼ˆUTF-8 ç¼–ç ï¼Œçº¦æ¯å­—ç¬¦ 2-3 å­—èŠ‚ï¼Œè¿™é‡Œå– 2.5ï¼‰\n    text_size_bytes = total_chars * 2.5\n    text_size_mb = text_size_bytes / (1024 * 1024)\n    \n    # è®¡ç®—å…ƒæ•°æ®å­˜å‚¨å¤§å°ï¼ˆä¼°ç®—æ¯ä¸ª chunk çº¦ 500 å­—èŠ‚å…ƒæ•°æ®ï¼‰\n    metadata_size_bytes = estimated_chunks * 500\n    metadata_size_mb = metadata_size_bytes / (1024 * 1024)\n    \n    # ç´¢å¼•å¼€é”€ï¼ˆFAISS æˆ–å…¶ä»–ç´¢å¼•ç»“æ„ï¼Œçº¦ä¸ºå‘é‡å¤§å°çš„ 20-30%ï¼‰\n    index_overhead_factor = 0.25\n    index_overhead_mb = vector_size_mb * index_overhead_factor\n    \n    # æ€»å¤§å°\n    total_size_mb = vector_size_mb + text_size_mb + metadata_size_mb + index_overhead_mb\n    \n    return {\n        'total_documents': len(documents),\n        'total_characters': total_chars,\n        'estimated_chunks': estimated_chunks,\n        'embedding_dimension': embedding_dim,\n        'vector_storage_mb': round(vector_size_mb, 2),\n        'text_storage_mb': round(text_size_mb, 2),\n        'metadata_storage_mb': round(metadata_size_mb, 2),\n        'index_overhead_mb': round(index_overhead_mb, 2),\n        'total_size_mb': round(total_size_mb, 2),\n        'total_size_gb': round(total_size_mb / 1024, 3)\n    }\n\n\ndef print_size_estimation(estimation):\n    \"\"\"æ‰“å°æ ¼å¼åŒ–çš„å¤§å°é¢„æµ‹ç»“æœ\"\"\"\n    if 'error' in estimation:\n        print(f\"âŒ {estimation['error']}\")\n        return\n    \n    print(\"=\" * 60)\n    print(\"ğŸ“Š å‘é‡æ•°æ®åº“å¤§å°é¢„æµ‹\")\n    print(\"=\" * 60)\n    print(f\"\\nğŸ“„ æ–‡æ¡£ç»Ÿè®¡:\")\n    print(f\"  - æ–‡æ¡£æ•°é‡: {estimation['total_documents']}\")\n    print(f\"  - æ€»å­—ç¬¦æ•°: {estimation['total_characters']:,}\")\n    print(f\"  - é¢„è®¡åˆ†å—æ•°: {estimation['estimated_chunks']:,}\")\n    print(f\"  - å‘é‡ç»´åº¦: {estimation['embedding_dimension']}\")\n    \n    print(f\"\\nğŸ’¾ å­˜å‚¨ç©ºé—´é¢„æµ‹:\")\n    print(f\"  - å‘é‡å­˜å‚¨: {estimation['vector_storage_mb']:,.2f} MB\")\n    print(f\"  - æ–‡æœ¬å­˜å‚¨: {estimation['text_storage_mb']:,.2f} MB\")\n    print(f\"  - å…ƒæ•°æ®å­˜å‚¨: {estimation['metadata_storage_mb']:,.2f} MB\")\n    print(f\"  - ç´¢å¼•å¼€é”€: {estimation['index_overhead_mb']:,.2f} MB\")\n    \n    print(f\"\\nğŸ“¦ æ€»è®¡:\")\n    print(f\"  - æ€»å¤§å°: {estimation['total_size_mb']:,.2f} MB ({estimation['total_size_gb']:.3f} GB)\")\n    \n    # æ·»åŠ å»ºè®®\n    total_mb = estimation['total_size_mb']\n    print(f\"\\nğŸ’¡ å»ºè®®:\")\n    if total_mb < 100:\n        print(\"  âœ… å†…å­˜å ç”¨è¾ƒå°ï¼Œå¯ä»¥è½»æ¾åœ¨å†…å­˜ä¸­å¤„ç†\")\n    elif total_mb < 1000:\n        print(\"  âš ï¸  å†…å­˜å ç”¨é€‚ä¸­ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„å¯ç”¨å†…å­˜\")\n    else:\n        print(\"  âš ï¸  å†…å­˜å ç”¨è¾ƒå¤§ï¼Œå»ºè®®è€ƒè™‘:\")\n        print(\"     - ä½¿ç”¨æŒä¹…åŒ–å‘é‡æ•°æ®åº“ï¼ˆå¦‚ Chromaã€Weaviateï¼‰\")\n        print(\"     - åˆ†æ‰¹å¤„ç†æ–‡æ¡£\")\n        print(\"     - ä½¿ç”¨æ›´å¼ºå¤§çš„æœåŠ¡å™¨\")\n    \n    print(\"=\" * 60)\n\n\n# å¦‚æœå·²åŠ è½½æ–‡æ¡£ï¼Œè¿›è¡Œå¤§å°é¢„æµ‹\nif 'documents' in locals() and documents:\n    print(\"ğŸ”„ æ­£åœ¨åˆ†ææ–‡æ¡£å¹¶é¢„æµ‹å‘é‡æ•°æ®åº“å¤§å°...\\n\")\n    \n    estimation = estimate_vector_db_size(documents)\n    print_size_estimation(estimation)\n    \n    # ä¿å­˜é¢„æµ‹ç»“æœä¾›åç»­å‚è€ƒ\n    db_size_estimation = estimation\n    \nelse:\n    print(\"âš ï¸  è¯·å…ˆè¿è¡Œä¸Šä¸€ä¸ªå•å…ƒæ ¼åŠ è½½æ–‡æ¡£\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "build-index",
   "metadata": {},
   "source": [
    "## 5. æ„å»ºå‘é‡ç´¢å¼•\n",
    "\n",
    "ä½¿ç”¨ BGE-M3 æ¨¡å‹å¯¹æ–‡æ¡£è¿›è¡Œå‘é‡åŒ–ï¼Œå¹¶æ„å»ºç´¢å¼•ä»¥æ”¯æŒé«˜æ•ˆæ£€ç´¢ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-index",
   "metadata": {},
   "outputs": [],
   "source": "if 'documents' in locals() and documents:\n    # æ˜¾ç¤ºé¢„æµ‹ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰\n    if 'db_size_estimation' in locals():\n        print(f\"ğŸ“Š é¢„è®¡å‘é‡æ•°æ®åº“å¤§å°: {db_size_estimation['total_size_mb']:.2f} MB\")\n        print(f\"   é¢„è®¡å¤„ç† {db_size_estimation['estimated_chunks']:,} ä¸ªæ–‡æœ¬å—\\n\")\n    \n    print(\"ğŸ”„ å¼€å§‹æ„å»ºå‘é‡ç´¢å¼•...\")\n    print(\"   è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œå–å†³äºæ–‡æ¡£å¤§å°\\n\")\n    \n    try:\n        # æ„å»ºå‘é‡ç´¢å¼•\n        index = VectorStoreIndex.from_documents(\n            documents,\n            show_progress=True\n        )\n        \n        print(\"\\nâœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼\")\n        print(\"   ç°åœ¨å¯ä»¥è¿›è¡Œæ–‡æ¡£æŸ¥è¯¢äº†\")\n        \n        # å¦‚æœæœ‰é¢„æµ‹ä¿¡æ¯ï¼Œå¯ä»¥è¿›è¡Œæ¯”è¾ƒ\n        if 'db_size_estimation' in locals():\n            print(f\"\\nğŸ’¾ é¢„æµ‹çš„æ•°æ®åº“å¤§å°: {db_size_estimation['total_size_mb']:.2f} MB\")\n            print(\"   ï¼ˆå®é™…å†…å­˜ä½¿ç”¨å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼‰\")\n        \n    except Exception as e:\n        print(f\"âŒ æ„å»ºç´¢å¼•å¤±è´¥: {str(e)}\")\nelse:\n    print(\"âš ï¸  è¯·å…ˆè¿è¡Œä¸Šä¸€ä¸ªå•å…ƒæ ¼åŠ è½½æ–‡æ¡£\")"
  },
  {
   "cell_type": "markdown",
   "id": "query",
   "metadata": {},
   "source": [
    "## 6. åˆ›å»ºæŸ¥è¯¢å¼•æ“\n",
    "\n",
    "åˆ›å»ºæŸ¥è¯¢å¼•æ“ï¼Œæ”¯æŒåŸºäºæ–‡æ¡£å†…å®¹çš„æ™ºèƒ½é—®ç­”ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'index' in locals():\n",
    "    # åˆ›å»ºæŸ¥è¯¢å¼•æ“\n",
    "    query_engine = index.as_query_engine(\n",
    "        similarity_top_k=3,  # è¿”å›æœ€ç›¸ä¼¼çš„ 3 ä¸ªç‰‡æ®µ\n",
    "        response_mode=\"compact\"  # ä½¿ç”¨ç´§å‡‘æ¨¡å¼ç”Ÿæˆå›ç­”\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… æŸ¥è¯¢å¼•æ“åˆ›å»ºå®Œæˆ\")\n",
    "    print(\"\\nä½¿ç”¨æ–¹å¼:\")\n",
    "    print(\"  response = query_engine.query('ä½ çš„é—®é¢˜')\")\n",
    "    print(\"  print(response)\")\n",
    "else:\n",
    "    print(\"âš ï¸  è¯·å…ˆè¿è¡Œä¸Šä¸€ä¸ªå•å…ƒæ ¼æ„å»ºç´¢å¼•\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-queries",
   "metadata": {},
   "source": [
    "## 7. ç¤ºä¾‹æŸ¥è¯¢\n",
    "\n",
    "è¿è¡Œä¸€äº›ç¤ºä¾‹æŸ¥è¯¢æ¥æµ‹è¯• RAG ç³»ç»Ÿã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-example-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¤ºä¾‹æŸ¥è¯¢ 1\n",
    "if 'query_engine' in locals():\n",
    "    query = \"æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "    print(f\"ğŸ” æŸ¥è¯¢: {query}\\n\")\n",
    "    \n",
    "    try:\n",
    "        response = query_engine.query(query)\n",
    "        print(\"ğŸ’¡ å›ç­”:\")\n",
    "        print(response)\n",
    "        \n",
    "        print(\"\\nğŸ“š ç›¸å…³æ–‡æ¡£ç‰‡æ®µ:\")\n",
    "        for i, node in enumerate(response.source_nodes, 1):\n",
    "            print(f\"\\n  ç‰‡æ®µ {i} (ç›¸ä¼¼åº¦: {node.score:.4f}):\")\n",
    "            print(f\"  {node.text[:200]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}\")\n",
    "else:\n",
    "    print(\"âš ï¸  è¯·å…ˆåˆ›å»ºæŸ¥è¯¢å¼•æ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-example-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¤ºä¾‹æŸ¥è¯¢ 2 - è‡ªå®šä¹‰æŸ¥è¯¢\n",
    "if 'query_engine' in locals():\n",
    "    # ä¿®æ”¹è¿™é‡Œçš„æŸ¥è¯¢å†…å®¹\n",
    "    custom_query = \"è¯·æ€»ç»“æ–‡æ¡£ä¸­çš„å…³é”®è¦ç‚¹\"\n",
    "    \n",
    "    print(f\"ğŸ” æŸ¥è¯¢: {custom_query}\\n\")\n",
    "    \n",
    "    try:\n",
    "        response = query_engine.query(custom_query)\n",
    "        print(\"ğŸ’¡ å›ç­”:\")\n",
    "        print(response)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}\")\n",
    "else:\n",
    "    print(\"âš ï¸  è¯·å…ˆåˆ›å»ºæŸ¥è¯¢å¼•æ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interactive",
   "metadata": {},
   "source": [
    "## 8. äº¤äº’å¼æŸ¥è¯¢\n",
    "\n",
    "åˆ›å»ºä¸€ä¸ªç®€å•çš„äº¤äº’å¼ç•Œé¢ï¼Œæ–¹ä¾¿è¿›è¡Œå¤šè½®æŸ¥è¯¢ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive-query",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_query():\n",
    "    \"\"\"äº¤äº’å¼æŸ¥è¯¢å‡½æ•°\"\"\"\n",
    "    if 'query_engine' not in locals() and 'query_engine' not in globals():\n",
    "        print(\"âš ï¸  è¯·å…ˆåˆ›å»ºæŸ¥è¯¢å¼•æ“\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"ğŸ“– PDF RAG äº¤äº’å¼æŸ¥è¯¢ç³»ç»Ÿ\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nğŸ” è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: \").strip()\n",
    "            \n",
    "            if query.lower() in ['exit', 'quit', 'é€€å‡º']:\n",
    "                print(\"\\nğŸ‘‹ å†è§ï¼\")\n",
    "                break\n",
    "            \n",
    "            if not query:\n",
    "                continue\n",
    "            \n",
    "            print(\"\\nğŸ’­ æ€è€ƒä¸­...\\n\")\n",
    "            response = query_engine.query(query)\n",
    "            \n",
    "            print(\"ğŸ’¡ å›ç­”:\")\n",
    "            print(response)\n",
    "            print(\"\\n\" + \"-\"*50)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nğŸ‘‹ å†è§ï¼\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ æŸ¥è¯¢å‡ºé”™: {str(e)}\")\n",
    "\n",
    "# å–æ¶ˆæ³¨é‡Šä¸‹é¢è¿™è¡Œæ¥å¯åŠ¨äº¤äº’å¼æŸ¥è¯¢\n",
    "# interactive_query()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "## æ€»ç»“\n\næœ¬ Notebook å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ XPULink å¹³å°ä¸Šçš„ BGE-M3 Embedding æ¨¡å‹æ„å»ºä¸€ä¸ªå®Œæ•´çš„ PDF RAG ç³»ç»Ÿï¼š\n\n1. âœ… é…ç½®äº† BGE-M3 Embedding æ¨¡å‹\n2. âœ… åŠ è½½å’Œå¤„ç† PDF æ–‡æ¡£\n3. âœ… **é¢„æµ‹å‘é‡æ•°æ®åº“å¤§å°**ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰\n4. âœ… æ„å»ºäº†å‘é‡ç´¢å¼•\n5. âœ… å®ç°äº†æ™ºèƒ½æŸ¥è¯¢åŠŸèƒ½\n6. âœ… æä¾›äº†äº¤äº’å¼æŸ¥è¯¢ç•Œé¢\n\n### å‘é‡æ•°æ®åº“å¤§å°é¢„æµ‹åŠŸèƒ½\n\næ–°å¢çš„é¢„æµ‹åŠŸèƒ½å¯ä»¥å¸®åŠ©æ‚¨ï¼š\n- ğŸ“Š åœ¨æ„å»ºç´¢å¼•å‰é¢„ä¼°æ‰€éœ€çš„å­˜å‚¨ç©ºé—´\n- ğŸ’¾ äº†è§£æ–‡æ¡£ä¼šè¢«åˆ†æˆå¤šå°‘ä¸ªæ–‡æœ¬å—\n- ğŸ¯ è·å–é’ˆå¯¹ä¸åŒæ•°æ®è§„æ¨¡çš„ä¼˜åŒ–å»ºè®®\n- âš¡ æ›´å¥½åœ°è§„åˆ’èµ„æºå’Œé€‰æ‹©åˆé€‚çš„å‘é‡æ•°æ®åº“\n\né¢„æµ‹è€ƒè™‘äº†ä»¥ä¸‹å› ç´ ï¼š\n- å‘é‡å­˜å‚¨ç©ºé—´ï¼ˆBGE-M3 ä½¿ç”¨ 1024 ç»´å‘é‡ï¼‰\n- æ–‡æœ¬å†…å®¹å­˜å‚¨\n- å…ƒæ•°æ®å¼€é”€\n- ç´¢å¼•ç»“æ„å¼€é”€\n\n### ä¸‹ä¸€æ­¥\n\n- å¯ä»¥å°è¯•ä¸åŒçš„ PDF æ–‡æ¡£\n- è°ƒæ•´ `similarity_top_k` å‚æ•°æ¥æ§åˆ¶æ£€ç´¢çš„æ–‡æ¡£æ•°é‡\n- ä¿®æ”¹ LLM çš„ `temperature` å‚æ•°æ¥æ§åˆ¶å›ç­”çš„åˆ›é€ æ€§\n- æ ¹æ®é¢„æµ‹ç»“æœé€‰æ‹©åˆé€‚çš„å‘é‡æ•°æ®åº“æ–¹æ¡ˆ\n- é›†æˆåˆ°å®é™…åº”ç”¨ä¸­\n\n### ç›¸å…³èµ„æº\n\n- [XPULink å®˜ç½‘](https://www.xpulink.ai)\n- [LlamaIndex æ–‡æ¡£](https://docs.llamaindex.ai/)\n- [BGE-M3 æ¨¡å‹ä»‹ç»](https://github.com/FlagOpen/FlagEmbedding)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}