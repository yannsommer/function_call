"""
Qwen3-32B LoRA å¾®è°ƒç¤ºä¾‹

æœ¬è„šæœ¬å±•ç¤ºå¦‚ä½•ä½¿ç”¨ XPULink API å¯¹ Qwen3-32B æ¨¡å‹è¿›è¡Œ LoRA (Low-Rank Adaptation) å¾®è°ƒã€‚
LoRA æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•,å¯ä»¥ç”¨è¾ƒå°‘çš„è®¡ç®—èµ„æºå¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå®šåˆ¶åŒ–è®­ç»ƒã€‚

ä½œè€…: XPULink
æ—¥æœŸ: 2025-01
"""

import os
import json
import requests
import time
from typing import List, Dict, Optional
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class XPULinkLoRAFineTuner:
    """XPULink LoRA å¾®è°ƒç®¡ç†ç±»"""

    def __init__(self, api_key: Optional[str] = None, base_url: str = "https://www.xpulink.ai/v1"):
        """
        åˆå§‹åŒ– LoRA å¾®è°ƒå™¨

        Args:
            api_key: XPULink API Key (å¦‚æœä¸æä¾›,ä¼šä»ç¯å¢ƒå˜é‡ XPULINK_API_KEY è¯»å–)
            base_url: API åŸºç¡€ URL
        """
        self.api_key = api_key or os.getenv("XPULINK_API_KEY")
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ° API Key,è¯·è®¾ç½® XPULINK_API_KEY ç¯å¢ƒå˜é‡æˆ–ä¼ å…¥ api_key å‚æ•°")

        self.base_url = base_url.rstrip('/')
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

    def prepare_training_data(self, conversations: List[Dict[str, str]], output_file: str):
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®,è½¬æ¢ä¸º JSONL æ ¼å¼

        Args:
            conversations: å¯¹è¯åˆ—è¡¨,æ¯ä¸ªå¯¹è¯åŒ…å«å¤šè½®äº¤äº’
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        ç¤ºä¾‹å¯¹è¯æ ¼å¼:
        [
            {
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ?"},
                    {"role": "assistant", "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯..."}
                ]
            }
        ]
        """
        with open(output_file, 'w', encoding='utf-8') as f:
            for conversation in conversations:
                f.write(json.dumps(conversation, ensure_ascii=False) + '\n')

        print(f"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        print(f"ğŸ“Š æ€»å¯¹è¯æ•°: {len(conversations)}")
        return output_file

    def upload_training_file(self, file_path: str) -> str:
        """
        ä¸Šä¼ è®­ç»ƒæ–‡ä»¶åˆ° XPULink

        Args:
            file_path: è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„

        Returns:
            file_id: ä¸Šä¼ åçš„æ–‡ä»¶ ID
        """
        url = f"{self.base_url}/files"

        with open(file_path, 'rb') as f:
            files = {
                'file': (os.path.basename(file_path), f, 'application/json'),
                'purpose': (None, 'fine-tune')
            }

            # ç§»é™¤ Content-Type header,è®© requests è‡ªåŠ¨å¤„ç† multipart/form-data
            headers = {"Authorization": f"Bearer {self.api_key}"}

            response = requests.post(url, headers=headers, files=files, timeout=60)

            if response.status_code != 200:
                raise Exception(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {response.text}")

            result = response.json()
            file_id = result.get('id')

            print(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ!")
            print(f"ğŸ“„ æ–‡ä»¶ ID: {file_id}")

            return file_id

    def create_finetune_job(
        self,
        training_file_id: str,
        model: str = "qwen3-32b",
        suffix: Optional[str] = None,
        hyperparameters: Optional[Dict] = None
    ) -> str:
        """
        åˆ›å»º LoRA å¾®è°ƒä»»åŠ¡

        Args:
            training_file_id: è®­ç»ƒæ–‡ä»¶ ID
            model: åŸºç¡€æ¨¡å‹åç§°
            suffix: å¾®è°ƒæ¨¡å‹çš„åç¼€åç§°
            hyperparameters: è¶…å‚æ•°é…ç½®

        Returns:
            job_id: å¾®è°ƒä»»åŠ¡ ID
        """
        url = f"{self.base_url}/fine_tuning/jobs"

        # é»˜è®¤è¶…å‚æ•°
        default_hyperparams = {
            "n_epochs": 3,              # è®­ç»ƒè½®æ•°
            "batch_size": 4,            # æ‰¹æ¬¡å¤§å°
            "learning_rate": 5e-5,      # å­¦ä¹ ç‡
            "lora_r": 8,                # LoRA ç§©
            "lora_alpha": 16,           # LoRA alpha å‚æ•°
            "lora_dropout": 0.05        # LoRA dropout
        }

        if hyperparameters:
            default_hyperparams.update(hyperparameters)

        payload = {
            "training_file": training_file_id,
            "model": model,
            "hyperparameters": default_hyperparams
        }

        if suffix:
            payload["suffix"] = suffix

        response = requests.post(url, headers=self.headers, json=payload, timeout=30)

        if response.status_code not in [200, 201]:
            raise Exception(f"åˆ›å»ºå¾®è°ƒä»»åŠ¡å¤±è´¥: {response.text}")

        result = response.json()
        job_id = result.get('id')

        print(f"âœ… å¾®è°ƒä»»åŠ¡åˆ›å»ºæˆåŠŸ!")
        print(f"ğŸ†” ä»»åŠ¡ ID: {job_id}")
        print(f"ğŸ“Š è¶…å‚æ•°é…ç½®:")
        for key, value in default_hyperparams.items():
            print(f"   - {key}: {value}")

        return job_id

    def check_job_status(self, job_id: str) -> Dict:
        """
        æ£€æŸ¥å¾®è°ƒä»»åŠ¡çŠ¶æ€

        Args:
            job_id: å¾®è°ƒä»»åŠ¡ ID

        Returns:
            ä»»åŠ¡çŠ¶æ€ä¿¡æ¯
        """
        url = f"{self.base_url}/fine_tuning/jobs/{job_id}"

        response = requests.get(url, headers=self.headers, timeout=30)

        if response.status_code != 200:
            raise Exception(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {response.text}")

        return response.json()

    def wait_for_completion(self, job_id: str, check_interval: int = 30) -> Dict:
        """
        ç­‰å¾…å¾®è°ƒä»»åŠ¡å®Œæˆ

        Args:
            job_id: å¾®è°ƒä»»åŠ¡ ID
            check_interval: æ£€æŸ¥é—´éš”(ç§’)

        Returns:
            æœ€ç»ˆä»»åŠ¡çŠ¶æ€
        """
        print(f"â³ ç­‰å¾…å¾®è°ƒä»»åŠ¡å®Œæˆ... (ä»»åŠ¡ ID: {job_id})")

        while True:
            status = self.check_job_status(job_id)
            current_status = status.get('status')

            print(f"ğŸ“Š å½“å‰çŠ¶æ€: {current_status}")

            if current_status == 'succeeded':
                print(f"âœ… å¾®è°ƒä»»åŠ¡å®Œæˆ!")
                print(f"ğŸ‰ å¾®è°ƒæ¨¡å‹: {status.get('fine_tuned_model')}")
                return status

            elif current_status == 'failed':
                error = status.get('error', 'æœªçŸ¥é”™è¯¯')
                raise Exception(f"âŒ å¾®è°ƒä»»åŠ¡å¤±è´¥: {error}")

            elif current_status in ['cancelled', 'canceled']:
                raise Exception(f"âŒ å¾®è°ƒä»»åŠ¡å·²å–æ¶ˆ")

            # ä»åœ¨è¿›è¡Œä¸­
            time.sleep(check_interval)

    def list_finetune_jobs(self, limit: int = 10) -> List[Dict]:
        """
        åˆ—å‡ºå¾®è°ƒä»»åŠ¡

        Args:
            limit: è¿”å›çš„ä»»åŠ¡æ•°é‡é™åˆ¶

        Returns:
            å¾®è°ƒä»»åŠ¡åˆ—è¡¨
        """
        url = f"{self.base_url}/fine_tuning/jobs?limit={limit}"

        response = requests.get(url, headers=self.headers, timeout=30)

        if response.status_code != 200:
            raise Exception(f"è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {response.text}")

        return response.json().get('data', [])

    def test_finetuned_model(self, model_name: str, test_prompt: str, max_tokens: int = 200):
        """
        æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹

        Args:
            model_name: å¾®è°ƒæ¨¡å‹åç§°
            test_prompt: æµ‹è¯•æç¤ºè¯
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
        """
        url = f"{self.base_url}/chat/completions"

        payload = {
            "model": model_name,
            "messages": [
                {"role": "user", "content": test_prompt}
            ],
            "max_tokens": max_tokens,
            "temperature": 0.7
        }

        response = requests.post(url, headers=self.headers, json=payload, timeout=60)

        if response.status_code != 200:
            raise Exception(f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {response.text}")

        result = response.json()
        content = result["choices"][0]["message"]["content"]

        print(f"ğŸ¤– æ¨¡å‹å›ç­”:\n{content}")
        return content


def example_basic_usage():
    """åŸºç¡€ä½¿ç”¨ç¤ºä¾‹"""
    print("=" * 60)
    print("Qwen3-32B LoRA å¾®è°ƒåŸºç¡€ç¤ºä¾‹")
    print("=" * 60)

    # 1. åˆå§‹åŒ–å¾®è°ƒå™¨
    finetuner = XPULinkLoRAFineTuner()

    # 2. å‡†å¤‡è®­ç»ƒæ•°æ®
    training_data = [
        {
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonç¼–ç¨‹åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": "å¦‚ä½•åœ¨Pythonä¸­è¯»å–JSONæ–‡ä»¶?"},
                {"role": "assistant", "content": "åœ¨Pythonä¸­è¯»å–JSONæ–‡ä»¶å¯ä»¥ä½¿ç”¨jsonæ¨¡å—:\n\n```python\nimport json\n\nwith open('data.json', 'r', encoding='utf-8') as f:\n    data = json.load(f)\n```"}
            ]
        },
        {
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonç¼–ç¨‹åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": "ä»€ä¹ˆæ˜¯åˆ—è¡¨æ¨å¯¼å¼?"},
                {"role": "assistant", "content": "åˆ—è¡¨æ¨å¯¼å¼æ˜¯Pythonä¸­åˆ›å»ºåˆ—è¡¨çš„ç®€æ´æ–¹æ³•:\n\n```python\n# åŸºæœ¬è¯­æ³•\nnumbers = [x for x in range(10)]\n\n# å¸¦æ¡ä»¶\neven_numbers = [x for x in range(10) if x % 2 == 0]\n```"}
            ]
        },
        # å¯ä»¥æ·»åŠ æ›´å¤šè®­ç»ƒæ ·æœ¬...
    ]

    # ä¿å­˜è®­ç»ƒæ•°æ®
    data_file = finetuner.prepare_training_data(
        training_data,
        "LoRA/data/training_data.jsonl"
    )

    # 3. ä¸Šä¼ è®­ç»ƒæ–‡ä»¶
    print("\nğŸ“¤ ä¸Šä¼ è®­ç»ƒæ–‡ä»¶...")
    file_id = finetuner.upload_training_file(data_file)

    # 4. åˆ›å»ºå¾®è°ƒä»»åŠ¡
    print("\nğŸš€ åˆ›å»ºå¾®è°ƒä»»åŠ¡...")
    job_id = finetuner.create_finetune_job(
        training_file_id=file_id,
        model="qwen3-32b",
        suffix="python-assistant",
        hyperparameters={
            "n_epochs": 3,
            "batch_size": 2,
            "learning_rate": 1e-4,
            "lora_r": 8
        }
    )

    # 5. ç­‰å¾…å¾®è°ƒå®Œæˆ
    print("\nâ³ å¼€å§‹å¾®è°ƒ...")
    final_status = finetuner.wait_for_completion(job_id)

    # 6. æµ‹è¯•å¾®è°ƒæ¨¡å‹
    finetuned_model = final_status.get('fine_tuned_model')
    if finetuned_model:
        print(f"\nğŸ§ª æµ‹è¯•å¾®è°ƒæ¨¡å‹: {finetuned_model}")
        finetuner.test_finetuned_model(
            finetuned_model,
            "å¦‚ä½•åœ¨Pythonä¸­å¤„ç†å¼‚å¸¸?"
        )


def example_check_existing_jobs():
    """æ£€æŸ¥ç°æœ‰å¾®è°ƒä»»åŠ¡çš„ç¤ºä¾‹"""
    print("=" * 60)
    print("æŸ¥çœ‹ç°æœ‰å¾®è°ƒä»»åŠ¡")
    print("=" * 60)

    finetuner = XPULinkLoRAFineTuner()

    jobs = finetuner.list_finetune_jobs(limit=5)

    if not jobs:
        print("ğŸ“­ æš‚æ— å¾®è°ƒä»»åŠ¡")
        return

    print(f"ğŸ“‹ å…±æ‰¾åˆ° {len(jobs)} ä¸ªå¾®è°ƒä»»åŠ¡:\n")

    for i, job in enumerate(jobs, 1):
        print(f"{i}. ä»»åŠ¡ ID: {job.get('id')}")
        print(f"   çŠ¶æ€: {job.get('status')}")
        print(f"   æ¨¡å‹: {job.get('model')}")
        print(f"   åˆ›å»ºæ—¶é—´: {job.get('created_at')}")
        if job.get('fine_tuned_model'):
            print(f"   å¾®è°ƒæ¨¡å‹: {job.get('fine_tuned_model')}")
        print()


if __name__ == "__main__":
    # è¿è¡ŒåŸºç¡€ç¤ºä¾‹
    # æ³¨æ„: è¿™åªæ˜¯ç¤ºä¾‹ä»£ç ,å®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ® XPULink API çš„å®é™…æ¥å£è¿›è¡Œè°ƒæ•´

    print("""
    âš ï¸  ä½¿ç”¨å‰è¯·ç¡®ä¿:
    1. å·²è®¾ç½®ç¯å¢ƒå˜é‡ XPULINK_API_KEY
    2. å·²å‡†å¤‡å¥½è¶³å¤Ÿçš„è®­ç»ƒæ•°æ® (å»ºè®®è‡³å°‘ 50+ æ ·æœ¬)
    3. äº†è§£ XPULink å¹³å°çš„å¾®è°ƒ API æ–‡æ¡£

    æœ¬è„šæœ¬æä¾›çš„æ˜¯é€šç”¨çš„ LoRA å¾®è°ƒæµç¨‹ç¤ºä¾‹ã€‚
    å®é™… API æ¥å£å¯èƒ½æœ‰æ‰€ä¸åŒ,è¯·æ ¹æ®å®˜æ–¹æ–‡æ¡£è¿›è¡Œè°ƒæ•´ã€‚
    """)

    # å–æ¶ˆæ³¨é‡Šä»¥è¿è¡Œç¤ºä¾‹
    # example_basic_usage()
    # example_check_existing_jobs()
